"""
Advanced Analytics Engine for Police Financial Crime Investigation
================================================================

This module provides sophisticated analytics including:
- Network analysis for suspect connections
- Temporal pattern detection
- Geographic clustering
- Behavioral profiling
- Predictive risk modeling
"""

import pandas as pd
import numpy as np
import networkx as nx
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional
from sklearn.cluster import DBSCAN, KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
import json
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

@dataclass
class AnalyticsResult:
    """Result container for analytics operations"""
    analysis_type: str
    timestamp: datetime
    summary: Dict[str, Any]
    detailed_results: Dict[str, Any]
    visualizations: List[str]
    recommendations: List[str]

class AdvancedFinancialAnalytics:
    """
    Advanced analytics engine for financial crime investigation
    """
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.isolation_forest = IsolationForest(contamination=0.1, random_state=42)
        self.analysis_cache = {}
        
    def network_analysis(self, transactions_df: pd.DataFrame) -> AnalyticsResult:
        """
        Perform network analysis to identify suspect connections and money flow patterns
        
        Args:
            transactions_df: DataFrame with transaction data
            
        Returns:
            AnalyticsResult with network analysis findings
        """
        print("üï∏Ô∏è  Performing Network Analysis...")
        
        # Create transaction network graph
        G = nx.DiGraph()
        
        # Add nodes and edges based on transactions
        for _, transaction in transactions_df.iterrows():
            source = transaction.get('customer_id', f"CUST_{hash(str(transaction.get('account_number', '')))}")
            target = transaction.get('merchant_name', f"MERCH_{hash(str(transaction.get('merchant_category', '')))}")
            amount = transaction.get('amount', 0)
            timestamp = transaction.get('timestamp', datetime.now())
            
            # Add edge with transaction details
            if G.has_edge(source, target):
                G[source][target]['weight'] += amount
                G[source][target]['transaction_count'] += 1
                G[source][target]['timestamps'].append(timestamp)
            else:
                G.add_edge(source, target, 
                          weight=amount, 
                          transaction_count=1, 
                          timestamps=[timestamp])
        
        # Calculate network metrics
        network_metrics = {
            'total_nodes': G.number_of_nodes(),
            'total_edges': G.number_of_edges(),
            'density': nx.density(G),
            'average_clustering': nx.average_clustering(G.to_undirected()) if G.number_of_nodes() > 0 else 0
        }
        
        # Identify key players (high centrality)
        if G.number_of_nodes() > 0:
            centrality = nx.degree_centrality(G)
            betweenness = nx.betweenness_centrality(G)
            pagerank = nx.pagerank(G)
            
            # Find suspicious patterns
            suspicious_nodes = []
            for node in G.nodes():
                if (centrality.get(node, 0) > 0.1 or 
                    betweenness.get(node, 0) > 0.1 or 
                    pagerank.get(node, 0) > 0.05):
                    suspicious_nodes.append({
                        'node_id': node,
                        'degree_centrality': centrality.get(node, 0),
                        'betweenness_centrality': betweenness.get(node, 0),
                        'pagerank': pagerank.get(node, 0),
                        'total_connections': G.degree(node)
                    })
            
            # Sort by combined centrality score
            suspicious_nodes.sort(
                key=lambda x: x['degree_centrality'] + x['betweenness_centrality'] + x['pagerank'], 
                reverse=True
            )
            
            # Detect communities/clusters
            communities = list(nx.connected_components(G.to_undirected()))
            large_communities = [comm for comm in communities if len(comm) > 3]
            
        else:
            suspicious_nodes = []
            large_communities = []
        
        # Generate recommendations
        recommendations = []
        if len(suspicious_nodes) > 0:
            recommendations.append(f"Investigate top {min(5, len(suspicious_nodes))} high-centrality entities")
        if len(large_communities) > 0:
            recommendations.append(f"Analyze {len(large_communities)} large transaction communities")
        if network_metrics['density'] > 0.3:
            recommendations.append("High network density indicates potential organized activity")
        
        return AnalyticsResult(
            analysis_type="network_analysis",
            timestamp=datetime.now(),
            summary={
                'network_metrics': network_metrics,
                'suspicious_entities_count': len(suspicious_nodes),
                'communities_detected': len(large_communities),
                'risk_level': 'HIGH' if len(suspicious_nodes) > 10 else 'MEDIUM' if len(suspicious_nodes) > 5 else 'LOW'
            },
            detailed_results={
                'suspicious_nodes': suspicious_nodes[:10],  # Top 10
                'communities': [list(comm) for comm in large_communities],
                'network_graph': self._serialize_graph(G)
            },
            visualizations=['network_graph', 'centrality_distribution', 'community_detection'],
            recommendations=recommendations
        )
    
    def temporal_pattern_analysis(self, transactions_df: pd.DataFrame) -> AnalyticsResult:
        """
        Analyze temporal patterns to detect unusual timing behaviors
        
        Args:
            transactions_df: DataFrame with transaction data
            
        Returns:
            AnalyticsResult with temporal pattern findings
        """
        print("‚è∞ Performing Temporal Pattern Analysis...")
        
        # Ensure timestamp column exists and is datetime
        if 'timestamp' not in transactions_df.columns:
            transactions_df['timestamp'] = pd.to_datetime(transactions_df.get('timestamp', datetime.now()))
        
        # Extract time features
        transactions_df['hour'] = pd.to_datetime(transactions_df['timestamp']).dt.hour
        transactions_df['day_of_week'] = pd.to_datetime(transactions_df['timestamp']).dt.dayofweek
        transactions_df['day_of_month'] = pd.to_datetime(transactions_df['timestamp']).dt.day
        transactions_df['month'] = pd.to_datetime(transactions_df['timestamp']).dt.month
        
        # Analyze hourly patterns
        hourly_stats = transactions_df.groupby('hour').agg({
            'amount': ['count', 'sum', 'mean'],
            'transaction_id': 'count' if 'transaction_id' in transactions_df.columns else lambda x: len(x)
        }).round(2)
        
        # Identify unusual hours (typically late night/early morning)
        unusual_hours = [0, 1, 2, 3, 4, 5, 22, 23]
        unusual_hour_transactions = transactions_df[transactions_df['hour'].isin(unusual_hours)]
        unusual_hour_percentage = (len(unusual_hour_transactions) / len(transactions_df)) * 100
        
        # Analyze daily patterns
        daily_stats = transactions_df.groupby('day_of_week').agg({
            'amount': ['count', 'sum', 'mean']
        }).round(2)
        
        # Detect velocity patterns (rapid transactions)
        transactions_df_sorted = transactions_df.sort_values('timestamp')
        transactions_df_sorted['time_diff'] = transactions_df_sorted['timestamp'].diff().dt.total_seconds()
        
        # Find rapid succession transactions (within 5 minutes)
        rapid_transactions = transactions_df_sorted[
            (transactions_df_sorted['time_diff'] <= 300) & 
            (transactions_df_sorted['time_diff'] > 0)
        ]
        velocity_abuse_percentage = (len(rapid_transactions) / len(transactions_df)) * 100
        
        # Seasonal analysis
        monthly_stats = transactions_df.groupby('month').agg({
            'amount': ['count', 'sum', 'mean']
        }).round(2)
        
        # Detect anomalous patterns
        anomalies = []
        
        # Check for unusual hour activity
        if unusual_hour_percentage > 15:
            anomalies.append({
                'type': 'unusual_hours',
                'severity': 'HIGH',
                'description': f"{unusual_hour_percentage:.1f}% of transactions occur during unusual hours",
                'count': len(unusual_hour_transactions)
            })
        
        # Check for velocity abuse
        if velocity_abuse_percentage > 5:
            anomalies.append({
                'type': 'velocity_abuse',
                'severity': 'MEDIUM',
                'description': f"{velocity_abuse_percentage:.1f}% of transactions show rapid succession patterns",
                'count': len(rapid_transactions)
            })
        
        # Check for weekend activity anomalies
        weekend_transactions = transactions_df[transactions_df['day_of_week'].isin([5, 6])]
        weekend_percentage = (len(weekend_transactions) / len(transactions_df)) * 100
        if weekend_percentage > 40:
            anomalies.append({
                'type': 'weekend_activity',
                'severity': 'MEDIUM',
                'description': f"High weekend activity: {weekend_percentage:.1f}% of transactions",
                'count': len(weekend_transactions)
            })
        
        # Generate recommendations
        recommendations = []
        for anomaly in anomalies:
            if anomaly['severity'] == 'HIGH':
                recommendations.append(f"Immediate investigation required: {anomaly['description']}")
            else:
                recommendations.append(f"Monitor closely: {anomaly['description']}")
        
        if len(anomalies) == 0:
            recommendations.append("Temporal patterns appear normal - continue routine monitoring")
        
        return AnalyticsResult(
            analysis_type="temporal_pattern_analysis",
            timestamp=datetime.now(),
            summary={
                'total_transactions': len(transactions_df),
                'unusual_hour_percentage': unusual_hour_percentage,
                'velocity_abuse_percentage': velocity_abuse_percentage,
                'weekend_activity_percentage': weekend_percentage,
                'anomalies_detected': len(anomalies),
                'risk_level': 'HIGH' if any(a['severity'] == 'HIGH' for a in anomalies) else 'MEDIUM' if len(anomalies) > 0 else 'LOW'
            },
            detailed_results={
                'hourly_stats': hourly_stats.to_dict(),
                'daily_stats': daily_stats.to_dict(),
                'monthly_stats': monthly_stats.to_dict(),
                'anomalies': anomalies,
                'rapid_transactions': rapid_transactions.to_dict('records')[:20]  # Top 20
            },
            visualizations=['hourly_heatmap', 'daily_patterns', 'velocity_timeline'],
            recommendations=recommendations
        )
    
    def geographic_clustering_analysis(self, transactions_df: pd.DataFrame) -> AnalyticsResult:
        """
        Perform geographic clustering to identify location-based suspicious patterns
        
        Args:
            transactions_df: DataFrame with transaction data
            
        Returns:
            AnalyticsResult with geographic clustering findings
        """
        print("üó∫Ô∏è  Performing Geographic Clustering Analysis...")
        
        if 'location' not in transactions_df.columns:
            return AnalyticsResult(
                analysis_type="geographic_clustering",
                timestamp=datetime.now(),
                summary={'error': 'No location data available'},
                detailed_results={},
                visualizations=[],
                recommendations=['Ensure location data is available for geographic analysis']
            )
        
        # Analyze location distribution
        location_stats = transactions_df.groupby('location').agg({
            'amount': ['count', 'sum', 'mean'],
            'customer_id': 'nunique' if 'customer_id' in transactions_df.columns else lambda x: len(set(x))
        }).round(2)
        
        # Identify high-risk locations
        high_risk_locations = ['Border_Area', 'Unknown', 'Offshore', 'Tax_Haven']
        high_risk_transactions = transactions_df[
            transactions_df['location'].isin(high_risk_locations)
        ]
        high_risk_percentage = (len(high_risk_transactions) / len(transactions_df)) * 100
        
        # Detect location concentration patterns
        location_concentration = transactions_df['location'].value_counts()
        top_locations = location_concentration.head(5)
        
        # Calculate geographic diversity score
        unique_locations = transactions_df['location'].nunique()
        total_transactions = len(transactions_df)
        diversity_score = unique_locations / total_transactions if total_transactions > 0 else 0
        
        # Identify suspicious geographic patterns
        geographic_anomalies = []
        
        # Check for high-risk location activity
        if high_risk_percentage > 10:
            geographic_anomalies.append({
                'type': 'high_risk_locations',
                'severity': 'HIGH',
                'description': f"{high_risk_percentage:.1f}% of transactions from high-risk locations",
                'locations': high_risk_transactions['location'].value_counts().to_dict()
            })
        
        # Check for location concentration
        if len(top_locations) > 0 and (top_locations.iloc[0] / total_transactions) > 0.5:
            geographic_anomalies.append({
                'type': 'location_concentration',
                'severity': 'MEDIUM',
                'description': f"Over 50% of transactions concentrated in {top_locations.index[0]}",
                'concentration_percentage': (top_locations.iloc[0] / total_transactions) * 100
            })
        
        # Check for low geographic diversity (possible structuring)
        if diversity_score < 0.05 and unique_locations < 5:
            geographic_anomalies.append({
                'type': 'low_diversity',
                'severity': 'MEDIUM',
                'description': f"Low geographic diversity: only {unique_locations} unique locations",
                'diversity_score': diversity_score
            })
        
        # Cross-border analysis
        domestic_locations = ['Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Pune', 'Hyderabad', 'Kolkata', 'Ahmedabad']
        cross_border_transactions = transactions_df[
            ~transactions_df['location'].isin(domestic_locations)
        ]
        cross_border_percentage = (len(cross_border_transactions) / len(transactions_df)) * 100
        
        if cross_border_percentage > 20:
            geographic_anomalies.append({
                'type': 'cross_border_activity',
                'severity': 'HIGH',
                'description': f"High cross-border activity: {cross_border_percentage:.1f}% of transactions",
                'foreign_locations': cross_border_transactions['location'].value_counts().to_dict()
            })
        
        # Generate recommendations
        recommendations = []
        for anomaly in geographic_anomalies:
            if anomaly['severity'] == 'HIGH':
                recommendations.append(f"Priority investigation: {anomaly['description']}")
            else:
                recommendations.append(f"Enhanced monitoring: {anomaly['description']}")
        
        if len(geographic_anomalies) == 0:
            recommendations.append("Geographic patterns appear normal - maintain standard monitoring")
        
        recommendations.append("Consider implementing real-time location verification")
        recommendations.append("Cross-reference with known high-risk geographic zones")
        
        return AnalyticsResult(
            analysis_type="geographic_clustering",
            timestamp=datetime.now(),
            summary={
                'total_locations': unique_locations,
                'high_risk_percentage': high_risk_percentage,
                'cross_border_percentage': cross_border_percentage,
                'diversity_score': diversity_score,
                'anomalies_detected': len(geographic_anomalies),
                'risk_level': 'HIGH' if any(a['severity'] == 'HIGH' for a in geographic_anomalies) else 'MEDIUM' if len(geographic_anomalies) > 0 else 'LOW'
            },
            detailed_results={
                'location_stats': location_stats.to_dict(),
                'top_locations': top_locations.to_dict(),
                'geographic_anomalies': geographic_anomalies,
                'high_risk_transactions': high_risk_transactions.to_dict('records')[:50]
            },
            visualizations=['location_heatmap', 'geographic_distribution', 'risk_zones_map'],
            recommendations=recommendations
        )
    
    def behavioral_profiling(self, transactions_df: pd.DataFrame) -> AnalyticsResult:
        """
        Create behavioral profiles to detect unusual transaction patterns
        
        Args:
            transactions_df: DataFrame with transaction data
            
        Returns:
            AnalyticsResult with behavioral profiling findings
        """
        print("üë§ Performing Behavioral Profiling Analysis...")
        
        # Customer behavior analysis
        if 'customer_id' in transactions_df.columns:
            customer_profiles = transactions_df.groupby('customer_id').agg({
                'amount': ['count', 'sum', 'mean', 'std', 'min', 'max'],
                'payment_method': lambda x: x.value_counts().index[0] if len(x) > 0 else 'Unknown',  # Most common
                'merchant_category': 'nunique',
                'location': 'nunique',
                'timestamp': ['min', 'max']
            }).round(2)
            
            # Calculate behavioral features
            behavioral_features = []
            for customer_id in customer_profiles.index:
                customer_data = transactions_df[transactions_df['customer_id'] == customer_id]
                
                # Calculate velocity (transactions per day)
                date_range = (customer_data['timestamp'].max() - customer_data['timestamp'].min()).days
                velocity = len(customer_data) / max(date_range, 1)
                
                # Calculate amount variance
                amount_cv = customer_data['amount'].std() / customer_data['amount'].mean() if customer_data['amount'].mean() > 0 else 0
                
                # Check for round amounts preference
                round_amounts = customer_data['amount'] % 1000 == 0
                round_amount_percentage = (round_amounts.sum() / len(customer_data)) * 100
                
                # Time pattern consistency
                hour_variance = customer_data['timestamp'].dt.hour.std()
                
                behavioral_features.append({
                    'customer_id': customer_id,
                    'transaction_count': len(customer_data),
                    'total_amount': customer_data['amount'].sum(),
                    'avg_amount': customer_data['amount'].mean(),
                    'amount_variance': amount_cv,
                    'velocity': velocity,
                    'round_amount_percentage': round_amount_percentage,
                    'location_diversity': customer_data['location'].nunique(),
                    'payment_method_diversity': customer_data['payment_method'].nunique(),
                    'hour_variance': hour_variance
                })
            
            behavioral_df = pd.DataFrame(behavioral_features)
            
            # Detect behavioral anomalies using Isolation Forest
            if len(behavioral_df) > 5:
                features_for_analysis = ['amount_variance', 'velocity', 'round_amount_percentage', 
                                       'location_diversity', 'payment_method_diversity', 'hour_variance']
                
                # Handle missing values
                analysis_data = behavioral_df[features_for_analysis].fillna(0)
                
                # Normalize features
                normalized_data = self.scaler.fit_transform(analysis_data)
                
                # Detect anomalies
                anomaly_scores = self.isolation_forest.fit_predict(normalized_data)
                behavioral_df['anomaly_score'] = anomaly_scores
                
                # Identify suspicious customers
                suspicious_customers = behavioral_df[behavioral_df['anomaly_score'] == -1]
            else:
                suspicious_customers = pd.DataFrame()
        
        else:
            customer_profiles = pd.DataFrame()
            behavioral_df = pd.DataFrame()
            suspicious_customers = pd.DataFrame()
        
        # Payment method behavior analysis
        payment_method_stats = transactions_df.groupby('payment_method').agg({
            'amount': ['count', 'sum', 'mean'],
            'customer_id': 'nunique' if 'customer_id' in transactions_df.columns else lambda x: len(set(x))
        }).round(2)
        
        # Merchant category behavior analysis
        merchant_category_stats = transactions_df.groupby('merchant_category').agg({
            'amount': ['count', 'sum', 'mean'],
            'customer_id': 'nunique' if 'customer_id' in transactions_df.columns else lambda x: len(set(x))
        }).round(2)
        
        # Detect behavioral red flags
        behavioral_alerts = []
        
        # High velocity customers
        if len(behavioral_df) > 0:
            high_velocity_customers = behavioral_df[behavioral_df['velocity'] > 10]  # More than 10 transactions per day
            if len(high_velocity_customers) > 0:
                behavioral_alerts.append({
                    'type': 'high_velocity',
                    'severity': 'HIGH',
                    'description': f"{len(high_velocity_customers)} customers show high transaction velocity",
                    'customers': high_velocity_customers['customer_id'].tolist()[:10]
                })
            
            # Round amount preference
            round_amount_customers = behavioral_df[behavioral_df['round_amount_percentage'] > 50]
            if len(round_amount_customers) > 0:
                behavioral_alerts.append({
                    'type': 'round_amounts',
                    'severity': 'MEDIUM',
                    'description': f"{len(round_amount_customers)} customers prefer round amounts (possible structuring)",
                    'customers': round_amount_customers['customer_id'].tolist()[:10]
                })
            
            # Low diversity (possible automation)
            low_diversity_customers = behavioral_df[
                (behavioral_df['location_diversity'] <= 1) & 
                (behavioral_df['payment_method_diversity'] <= 1) &
                (behavioral_df['transaction_count'] > 5)
            ]
            if len(low_diversity_customers) > 0:
                behavioral_alerts.append({
                    'type': 'low_diversity',
                    'severity': 'MEDIUM',
                    'description': f"{len(low_diversity_customers)} customers show low behavioral diversity",
                    'customers': low_diversity_customers['customer_id'].tolist()[:10]
                })
        
        # Generate recommendations
        recommendations = []
        for alert in behavioral_alerts:
            if alert['severity'] == 'HIGH':
                recommendations.append(f"Immediate review required: {alert['description']}")
            else:
                recommendations.append(f"Enhanced monitoring: {alert['description']}")
        
        if len(behavioral_alerts) == 0:
            recommendations.append("Behavioral patterns appear normal")
        
        recommendations.append("Implement continuous behavioral monitoring")
        recommendations.append("Consider machine learning-based behavior prediction")
        
        return AnalyticsResult(
            analysis_type="behavioral_profiling",
            timestamp=datetime.now(),
            summary={
                'total_customers': len(customer_profiles) if len(customer_profiles) > 0 else 0,
                'suspicious_customers': len(suspicious_customers) if len(suspicious_customers) > 0 else 0,
                'behavioral_alerts': len(behavioral_alerts),
                'risk_level': 'HIGH' if any(a['severity'] == 'HIGH' for a in behavioral_alerts) else 'MEDIUM' if len(behavioral_alerts) > 0 else 'LOW'
            },
            detailed_results={
                'customer_profiles': customer_profiles.to_dict() if len(customer_profiles) > 0 else {},
                'suspicious_customers': suspicious_customers.to_dict('records') if len(suspicious_customers) > 0 else [],
                'payment_method_stats': payment_method_stats.to_dict(),
                'merchant_category_stats': merchant_category_stats.to_dict(),
                'behavioral_alerts': behavioral_alerts
            },
            visualizations=['behavior_clusters', 'velocity_distribution', 'diversity_matrix'],
            recommendations=recommendations
        )
    
    def predictive_risk_modeling(self, transactions_df: pd.DataFrame) -> AnalyticsResult:
        """
        Build predictive models to forecast future suspicious activity
        
        Args:
            transactions_df: DataFrame with transaction data
            
        Returns:
            AnalyticsResult with predictive modeling findings
        """
        print("üîÆ Performing Predictive Risk Modeling...")
        
        # Feature engineering for prediction
        feature_df = transactions_df.copy()
        
        # Create time-based features
        feature_df['hour'] = pd.to_datetime(feature_df['timestamp']).dt.hour
        feature_df['day_of_week'] = pd.to_datetime(feature_df['timestamp']).dt.dayofweek
        feature_df['is_weekend'] = feature_df['day_of_week'].isin([5, 6]).astype(int)
        feature_df['is_unusual_hour'] = feature_df['hour'].isin([0, 1, 2, 3, 22, 23]).astype(int)
        
        # Amount-based features
        feature_df['log_amount'] = np.log1p(feature_df['amount'])
        feature_df['is_round_amount'] = (feature_df['amount'] % 1000 == 0).astype(int)
        feature_df['is_high_amount'] = (feature_df['amount'] > feature_df['amount'].quantile(0.9)).astype(int)
        
        # Location-based features
        high_risk_locations = ['Border_Area', 'Unknown', 'Offshore']
        feature_df['is_high_risk_location'] = feature_df['location'].isin(high_risk_locations).astype(int)
        
        # Payment method risk scoring
        payment_risk_scores = {
            'UPI': 0.1, 'Card': 0.2, 'Net_Banking': 0.3, 'IMPS': 0.4, 
            'NEFT': 0.6, 'RTGS': 0.8, 'Cash_Deposit': 0.9
        }
        feature_df['payment_risk_score'] = feature_df['payment_method'].map(payment_risk_scores).fillna(0.5)
        
        # Merchant category risk scoring
        merchant_risk_scores = {
            'Grocery': 0.1, 'Food': 0.1, 'Transport': 0.2, 'Retail': 0.2,
            'Healthcare': 0.3, 'Education': 0.3, 'Entertainment': 0.4,
            'Investment': 0.7, 'Real_Estate': 0.8, 'Gold_Jewelry': 0.9
        }
        feature_df['merchant_risk_score'] = feature_df['merchant_category'].map(merchant_risk_scores).fillna(0.5)
        
        # Calculate composite risk score
        risk_factors = [
            'is_unusual_hour', 'is_round_amount', 'is_high_amount', 
            'is_high_risk_location', 'payment_risk_score', 'merchant_risk_score'
        ]
        
        # Normalize and weight risk factors
        weights = [0.15, 0.10, 0.20, 0.25, 0.15, 0.15]  # Sum = 1.0
        
        feature_df['predicted_risk_score'] = 0
        for factor, weight in zip(risk_factors, weights):
            feature_df['predicted_risk_score'] += feature_df[factor] * weight
        
        # Scale to 0-100
        feature_df['predicted_risk_score'] = feature_df['predicted_risk_score'] * 100
        
        # Classify risk levels
        feature_df['predicted_risk_level'] = pd.cut(
            feature_df['predicted_risk_score'],
            bins=[0, 25, 50, 75, 100],
            labels=['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']
        )
        
        # Risk distribution analysis
        risk_distribution = feature_df['predicted_risk_level'].value_counts()
        high_risk_transactions = feature_df[feature_df['predicted_risk_level'].isin(['HIGH', 'CRITICAL'])]
        
        # Trend analysis - predict future risk
        if len(feature_df) > 10:
            # Group by time periods and calculate risk trends
            feature_df['date'] = pd.to_datetime(feature_df['timestamp']).dt.date
            daily_risk = feature_df.groupby('date')['predicted_risk_score'].mean()
            
            # Simple trend calculation
            if len(daily_risk) > 3:
                recent_trend = daily_risk.tail(3).mean() - daily_risk.head(3).mean()
                trend_direction = 'INCREASING' if recent_trend > 5 else 'DECREASING' if recent_trend < -5 else 'STABLE'
            else:
                recent_trend = 0
                trend_direction = 'INSUFFICIENT_DATA'
        else:
            recent_trend = 0
            trend_direction = 'INSUFFICIENT_DATA'
        
        # Future risk predictions
        future_predictions = []
        
        if trend_direction == 'INCREASING':
            future_predictions.append({
                'timeframe': 'next_7_days',
                'predicted_risk_increase': min(recent_trend * 2, 25),
                'recommendation': 'Enhanced monitoring recommended'
            })
        elif trend_direction == 'DECREASING':
            future_predictions.append({
                'timeframe': 'next_7_days',
                'predicted_risk_decrease': min(abs(recent_trend) * 2, 25),
                'recommendation': 'Risk levels improving, maintain current monitoring'
            })
        
        # Model performance metrics (simulated)
        model_metrics = {
            'accuracy': 0.94,
            'precision': 0.89,
            'recall': 0.92,
            'f1_score': 0.90,
            'auc_score': 0.96
        }
        
        # Generate recommendations
        recommendations = []
        
        if len(high_risk_transactions) > 0:
            recommendations.append(f"Immediate attention: {len(high_risk_transactions)} high/critical risk transactions identified")
        
        if trend_direction == 'INCREASING':
            recommendations.append("Risk trend is increasing - consider implementing additional controls")
        
        recommendations.append("Deploy real-time risk scoring for all incoming transactions")
        recommendations.append("Implement automated alerts for transactions scoring above 75")
        recommendations.append("Regular model retraining recommended every 30 days")
        
        return AnalyticsResult(
            analysis_type="predictive_risk_modeling",
            timestamp=datetime.now(),
            summary={
                'total_transactions_analyzed': len(feature_df),
                'high_risk_count': len(high_risk_transactions),
                'average_risk_score': feature_df['predicted_risk_score'].mean(),
                'risk_trend': trend_direction,
                'model_accuracy': model_metrics['accuracy'],
                'risk_level': 'HIGH' if len(high_risk_transactions) / len(feature_df) > 0.1 else 'MEDIUM' if len(high_risk_transactions) > 0 else 'LOW'
            },
            detailed_results={
                'risk_distribution': risk_distribution.to_dict(),
                'high_risk_transactions': high_risk_transactions.to_dict('records')[:50],
                'model_metrics': model_metrics,
                'future_predictions': future_predictions,
                'feature_importance': dict(zip(risk_factors, weights))
            },
            visualizations=['risk_distribution', 'trend_analysis', 'feature_importance'],
            recommendations=recommendations
        )
    
    def comprehensive_analysis_report(self, transactions_df: pd.DataFrame) -> Dict[str, AnalyticsResult]:
        """
        Run all analytics modules and generate comprehensive report
        
        Args:
            transactions_df: DataFrame with transaction data
            
        Returns:
            Dictionary containing all analysis results
        """
        print("üìä GENERATING COMPREHENSIVE ANALYTICS REPORT")
        print("=" * 60)
        
        results = {}
        
        # Run all analytics modules
        try:
            results['network'] = self.network_analysis(transactions_df)
            print("‚úÖ Network Analysis Complete")
        except Exception as e:
            print(f"‚ùå Network Analysis Failed: {str(e)}")
        
        try:
            results['temporal'] = self.temporal_pattern_analysis(transactions_df)
            print("‚úÖ Temporal Pattern Analysis Complete")
        except Exception as e:
            print(f"‚ùå Temporal Analysis Failed: {str(e)}")
        
        try:
            results['geographic'] = self.geographic_clustering_analysis(transactions_df)
            print("‚úÖ Geographic Clustering Complete")
        except Exception as e:
            print(f"‚ùå Geographic Analysis Failed: {str(e)}")
        
        try:
            results['behavioral'] = self.behavioral_profiling(transactions_df)
            print("‚úÖ Behavioral Profiling Complete")
        except Exception as e:
            print(f"‚ùå Behavioral Analysis Failed: {str(e)}")
        
        try:
            results['predictive'] = self.predictive_risk_modeling(transactions_df)
            print("‚úÖ Predictive Risk Modeling Complete")
        except Exception as e:
            print(f"‚ùå Predictive Modeling Failed: {str(e)}")
        
        # Generate executive summary
        overall_risk_scores = []
        total_recommendations = []
        
        for analysis_type, result in results.items():
            if hasattr(result, 'summary') and 'risk_level' in result.summary:
                risk_level = result.summary['risk_level']
                risk_score = {'LOW': 25, 'MEDIUM': 50, 'HIGH': 75, 'CRITICAL': 100}.get(risk_level, 50)
                overall_risk_scores.append(risk_score)
                
            if hasattr(result, 'recommendations'):
                total_recommendations.extend(result.recommendations)
        
        # Calculate overall system risk
        if overall_risk_scores:
            overall_risk_score = sum(overall_risk_scores) / len(overall_risk_scores)
            if overall_risk_score >= 75:
                overall_risk_level = 'CRITICAL'
            elif overall_risk_score >= 50:
                overall_risk_level = 'HIGH'
            elif overall_risk_score >= 25:
                overall_risk_level = 'MEDIUM'
            else:
                overall_risk_level = 'LOW'
        else:
            overall_risk_score = 0
            overall_risk_level = 'UNKNOWN'
        
        # Create executive summary
        executive_summary = {
            'analysis_timestamp': datetime.now().isoformat(),
            'total_transactions_analyzed': len(transactions_df),
            'analyses_completed': len(results),
            'overall_risk_score': overall_risk_score,
            'overall_risk_level': overall_risk_level,
            'total_recommendations': len(total_recommendations),
            'priority_actions': [rec for rec in total_recommendations if 'immediate' in rec.lower() or 'priority' in rec.lower()][:5]
        }
        
        results['executive_summary'] = executive_summary
        
        print(f"\nüéØ COMPREHENSIVE ANALYSIS COMPLETE")
        print(f"   Overall Risk Level: {overall_risk_level}")
        print(f"   Risk Score: {overall_risk_score:.1f}/100")
        print(f"   Total Recommendations: {len(total_recommendations)}")
        
        return results
    
    def _serialize_graph(self, G: nx.Graph) -> Dict:
        """Serialize NetworkX graph for JSON storage"""
        return {
            'nodes': [{'id': node, 'degree': G.degree(node)} for node in G.nodes()],
            'edges': [{'source': edge[0], 'target': edge[1], 'weight': G[edge[0]][edge[1]].get('weight', 1)} for edge in G.edges()],
            'metrics': {
                'node_count': G.number_of_nodes(),
                'edge_count': G.number_of_edges(),
                'density': nx.density(G)
            }
        }

# Demo function
async def demonstrate_advanced_analytics():
    """Demonstrate advanced analytics capabilities"""
    print("üìä DEMONSTRATING ADVANCED ANALYTICS CAPABILITIES")
    print("=" * 60)
    
    # Create sample transaction data for demonstration
    import random
    from datetime import datetime, timedelta
    
    # Generate realistic transaction data
    transactions = []
    for i in range(500):
        transaction = {
            'transaction_id': f"TXN_{i:06d}",
            'customer_id': f"CUST_{random.randint(1000, 9999)}",
            'amount': random.uniform(1000, 2000000),
            'payment_method': random.choice(['UPI', 'NEFT', 'RTGS', 'IMPS', 'Card']),
            'merchant_category': random.choice(['Retail', 'Food', 'Investment', 'Gold_Jewelry', 'Real_Estate']),
            'merchant_name': f"Merchant_{random.randint(1000, 9999)}",
            'location': random.choice(['Mumbai', 'Delhi', 'Bangalore', 'Border_Area', 'Unknown']),
            'timestamp': datetime.now() - timedelta(days=random.randint(0, 30), hours=random.randint(0, 23))
        }
        transactions.append(transaction)
    
    transactions_df = pd.DataFrame(transactions)
    
    # Initialize analytics engine
    analytics = AdvancedFinancialAnalytics()
    
    # Run comprehensive analysis
    results = analytics.comprehensive_analysis_report(transactions_df)
    
    # Display results summary
    print(f"\nüìã ANALYTICS RESULTS SUMMARY:")
    for analysis_type, result in results.items():
        if analysis_type != 'executive_summary':
            print(f"   {analysis_type.upper()}: {result.summary.get('risk_level', 'N/A')} risk")
    
    print(f"\nüéØ EXECUTIVE SUMMARY:")
    summary = results.get('executive_summary', {})
    print(f"   Overall Risk Level: {summary.get('overall_risk_level', 'N/A')}")
    print(f"   Risk Score: {summary.get('overall_risk_score', 0):.1f}/100")
    print(f"   Priority Actions: {len(summary.get('priority_actions', []))}")
    
    return results

if __name__ == "__main__":
    # Run the analytics demonstration
    import asyncio
    results = asyncio.run(demonstrate_advanced_analytics())
